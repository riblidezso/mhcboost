{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the performance of Neural networks and Gradient boosted trees  , and a stacked ensemble of both in MHC class I binding prediction\n",
    "\n",
    "\n",
    "---\n",
    "### Background\n",
    "\n",
    "The prediction of MHC peptide binding is an important method in immunology. Data driven methods give best performance for this task because of complexity of the MHC molecules and the binding process. The most successful machine learning models were Neural networks in this area.  I have seen articles describing SVM models, but nobody has tried gradient boosted trees (as far as I know).\n",
    "\n",
    "\n",
    "In the last years the vast majority of Data science competitions were won with a framework called Gradient Boosted Trees. These models very frequently outperform neural networks in gereal machine learning tasks. \n",
    "\n",
    "These facts gave me the idea to compare the perfomance of  Neural networks  and Gradient boosted trees and in MHC class I peptide binding prediction.\n",
    "\n",
    "Also Neural networks and Gradient boosted trees are very different models, therefore ensembling them can give a really big boost to performance. So I tried to ensemble these models.\n",
    "\n",
    "\n",
    "----\n",
    "### Data\n",
    "\n",
    "#### Source\n",
    "I have downloaded MHC class I peptide binding data which has been used in this benchmark article ( http://www.ncbi.nlm.nih.gov/pubmed/25017736)  from the IEDB website.\n",
    "\n",
    "\n",
    "#### Encoding \n",
    "\n",
    "Input data is very simply coded as the folowing:\n",
    "- zero padded sequences to the longest peptide length\n",
    "- amino acids are encoded in a one hot scheme\n",
    "- species are encoded in a one hot scheme\n",
    "- HLA types are encoded in a one-hot scheme\n",
    "\n",
    "#### Target \n",
    "\n",
    "I predicted binary binding labels based on the IC > 500nM threshold.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Models\n",
    "\n",
    "#### Neural network\n",
    "\n",
    "I used a neural network architecture similar to the one used in NetMHCpan.\n",
    "- 1 hidden layer\n",
    "- 60 units in the hidden layer\n",
    "\n",
    "I used the Keras software library. http://keras.io\n",
    "\n",
    "\n",
    "#### Gradient boosted trees\n",
    "Some resources about the model:\n",
    "- http://xgboost.readthedocs.org/en/latest/model.html\n",
    "- http://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf\n",
    "    \n",
    "I used the XGBoost software library. http://xgboost.readthedocs.org/en/latest/\n",
    "\n",
    "\n",
    "#### Ensembling\n",
    "\n",
    "\n",
    "\n",
    "I used a stacking approach for ensembling. I used original features, and probabilities predicted by base models as input for the meta estimator. The same folds are used in both step to prevent label leakage.\n",
    "\n",
    "Original article about stacking.\n",
    "- http://www.sciencedirect.com/science/article/pii/S0893608005800231\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Methods\n",
    "\n",
    "\n",
    "I made 5-fold cross validation evaluations, with AUC for all the measurements.\n",
    "\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "### Conclusions\n",
    "\n",
    "\n",
    "Gradient boosted trees significantly outperform neural networks in this task. The ensemble of neural networks and gradient boosted trees gives significant improvement over the base models. NetMHCPan outperforms my simple models.\n",
    "\n",
    "\n",
    "\n",
    "Method | 5 fold CV\n",
    "--- | --- \n",
    "NN | \n",
    "Gradient boosted trees | \n",
    "Ensemble of Gradient boosted trees + NN | 0.9362 \n",
    "NetMHCpan | 0.937\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Future Work\n",
    "\n",
    "#### Training on similar data than NetMHCpan\n",
    "\n",
    "My models perform worse than NetMHCpan which is considered to be the state of the art model in MHC class I prediction. \n",
    "\n",
    "But NetMHCpan uses sophsticated encogind of the MHC molecules, a sophicsticated way for predicting different length peptides, adds random not binding data to inputs, and it is an ensemble of models. I think that Gradient Boosted trees trained on the data used for NetMHCpan training, and using ensembling could significanlty improve the current state of the art.\n",
    "\n",
    "\n",
    "\n",
    "----\n",
    "#### Notes\n",
    "\n",
    "- To run this notebook you need to \n",
    "    - download the dataset\n",
    "    - export the MHC_DATA variable\n",
    "    - and have keras,theno,xgboost,sklearn,pandas,numpy installed.\n",
    "    \n",
    "    \n",
    "- This notebook can be run as it is. Just click run all. \n",
    "    - It takes some time but on a stronger computer it should be less than 30 minutes.\n",
    "\n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "This notebook was created by Dezso Ribli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import xgboost as xgb\n",
    "from keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import glob\n",
    "\n",
    "#go to working dir\n",
    "import os\n",
    "os.chdir(os.environ['MHC_DIR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load train/blind test data as in the benchmark.\"\"\"\n",
    "    \n",
    "    #load the cross validation data\n",
    "    dataf='benchmark_mhci_reliability/binding/bd2009.1/bdata.2009.mhci.public.1.txt'\n",
    "    data=pd.read_csv(dataf,sep='\\t')\n",
    "     \n",
    "    #encode hla,species,amino acids,in a one hot scheme\n",
    "    X_hla=one_hot_encode(data.mhc.values,data.mhc.values)\n",
    "    X_species=one_hot_encode(data.species.values,data.species.values)\n",
    "    X_seq=encode_seq(data.sequence.values,data.sequence.values)\n",
    "    \n",
    "    #stack columns\n",
    "    X=np.column_stack([X_species,X_hla,X_seq,data.peptide_length.values])\n",
    "    \n",
    "    #make binding class labels\n",
    "    y=np.ones(len(data.meas))\n",
    "    y[data.meas.values>=500]=0\n",
    "\n",
    "    #permute arrays as they are ordered!!!\n",
    "    rng=np.random.RandomState(42)\n",
    "    perm=rng.permutation(len(X))\n",
    "    X,y=X[perm],y[perm]\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "\n",
    "def one_hot_encode(x_all,x_in):\n",
    "    \"\"\"One hot encode categorical variable.\"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(x_all)\n",
    "    x_out=lb.transform( x_in )\n",
    "    return x_out\n",
    "\n",
    "\n",
    "def encode_seq(x_all,x_in):\n",
    "    \"\"\"Encode string amino acid sequences to numbers.\"\"\"\n",
    "    #zero pad peptids to equal length\n",
    "    maxlen=np.max(map(len,x_all))\n",
    "    x_temp_all=np.array([list(x.zfill(maxlen)) for x in x_all])\n",
    "    x_temp_in=np.array([list(x.zfill(maxlen)) for x in x_in])\n",
    "    \n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(x_temp_all.flatten())\n",
    "    x_out=np.column_stack([lb.transform(x_temp_in[:,i]) for i in range(maxlen)])\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "\n",
    "### Cross validation scheme for both models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cv(fitter,model_spec,X,y,n_folds=5):\n",
    "    \"\"\"Evaluate XGB model with cross validation.\"\"\"\n",
    "    #result\n",
    "    y_pred=np.zeros(len(y))\n",
    "    #loop folds\n",
    "    for train_index,test_index in KFold(len(X),n_folds=n_folds):\n",
    "        #train and predict\n",
    "        y_pred[test_index]=fitter(model_spec,X[train_index],y[train_index],X[test_index])\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def keras_fit_predict(get_model,X_train,y_train,X_test,\n",
    "                      validation_split=0.1,patience=1,nb_epoch=100,**kwargs):\n",
    "    \"\"\"Fit model on train test set with early stopping.\"\"\"\n",
    "    #get model\n",
    "    model=get_model(X_train.shape[1])\n",
    "    \n",
    "    #callbacks\n",
    "    best_model=ModelCheckpoint('best_model',save_best_only=True,verbose=2)\n",
    "    early_stop=EarlyStopping(patience=patience,verbose=2)\n",
    "    \n",
    "    #train it\n",
    "    model.fit(X_train,y_train,nb_epoch = nb_epoch,validation_split=validation_split,\n",
    "              callbacks=[best_model,early_stop],verbose=2,**kwargs)\n",
    "    \n",
    "    #load best model and predict\n",
    "    model.load_weights('best_model')\n",
    "    y_pred_test=model.predict(X_test).ravel()\n",
    "    \n",
    "    return y_pred_test\n",
    "\n",
    "def xgb_fit_predict(params,X_train,y_train,X_test,num_boost_round=5000,verbose_eval=500,\n",
    "                    early_stopping_rounds=200,validation_size=0.1):\n",
    "    \"\"\"Fit model on train test set with early stopping.\"\"\"\n",
    "    #split validation data for early stopping\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=validation_size)\n",
    "    \n",
    "    #convert to data format for xgb\n",
    "    dtrain = xgb.DMatrix( X_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix( X_valid, label=y_valid)\n",
    "    dtest = xgb.DMatrix( X_test )\n",
    "    \n",
    "    #define printed evaluations\n",
    "    evallist  = [(dtrain,'train'),(dvalid,'eval')]\n",
    "\n",
    "    #lets train\n",
    "    bst = xgb.train(params,dtrain,evals=evallist,\n",
    "                    num_boost_round=num_boost_round,\n",
    "                    early_stopping_rounds=early_stopping_rounds,\n",
    "                    verbose_eval=verbose_eval)\n",
    "    \n",
    "    #make predictions\n",
    "    y_pred_test=bst.predict(dtest)\n",
    "    \n",
    "    return y_pred_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X,y=load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train NN in a CV scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 99110 samples, validate on 11013 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_loss improved from inf to 0.37779, saving model to best_model\n",
      "46s - loss: 0.4517 - val_loss: 0.3778\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_loss improved from 0.37779 to 0.31539, saving model to best_model\n",
      "59s - loss: 0.3344 - val_loss: 0.3154\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_loss improved from 0.31539 to 0.31414, saving model to best_model\n",
      "66s - loss: 0.2848 - val_loss: 0.3141\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_loss improved from 0.31414 to 0.29760, saving model to best_model\n",
      "69s - loss: 0.2521 - val_loss: 0.2976\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_loss did not improve\n",
      "73s - loss: 0.2261 - val_loss: 0.3009\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_loss did not improve\n",
      "Epoch 00005: early stopping\n",
      "73s - loss: 0.2033 - val_loss: 0.3061\n",
      "Train on 99110 samples, validate on 11013 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_loss improved from inf to 0.36627, saving model to best_model\n",
      "49s - loss: 0.4510 - val_loss: 0.3663\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_loss improved from 0.36627 to 0.32085, saving model to best_model\n",
      "60s - loss: 0.3321 - val_loss: 0.3208\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_loss improved from 0.32085 to 0.30643, saving model to best_model\n",
      "66s - loss: 0.2832 - val_loss: 0.3064\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_loss improved from 0.30643 to 0.30066, saving model to best_model\n",
      "70s - loss: 0.2513 - val_loss: 0.3007\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_loss improved from 0.30066 to 0.29805, saving model to best_model\n",
      "71s - loss: 0.2250 - val_loss: 0.2980\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_loss did not improve\n",
      "73s - loss: 0.2026 - val_loss: 0.3141\n",
      "Epoch 7/100\n",
      "Epoch 00006: val_loss did not improve\n",
      "Epoch 00006: early stopping\n",
      "73s - loss: 0.1798 - val_loss: 0.3172\n",
      "Train on 99110 samples, validate on 11013 samples\n",
      "Epoch 1/100\n",
      "Epoch 00000: val_loss improved from inf to 0.37157, saving model to best_model\n",
      "45s - loss: 0.4535 - val_loss: 0.3716\n",
      "Epoch 2/100\n",
      "Epoch 00001: val_loss improved from 0.37157 to 0.32729, saving model to best_model\n",
      "56s - loss: 0.3362 - val_loss: 0.3273\n",
      "Epoch 3/100\n",
      "Epoch 00002: val_loss improved from 0.32729 to 0.31264, saving model to best_model\n",
      "61s - loss: 0.2867 - val_loss: 0.3126\n",
      "Epoch 4/100\n",
      "Epoch 00003: val_loss improved from 0.31264 to 0.31174, saving model to best_model\n",
      "68s - loss: 0.2548 - val_loss: 0.3117\n",
      "Epoch 5/100\n",
      "Epoch 00004: val_loss improved from 0.31174 to 0.30939, saving model to best_model\n",
      "73s - loss: 0.2264 - val_loss: 0.3094\n",
      "Epoch 6/100\n",
      "Epoch 00005: val_loss improved from 0.30939 to 0.30898, saving model to best_model"
     ]
    }
   ],
   "source": [
    "#create a very simple NN model\n",
    "def get_model(input_dim,n_dense=1024):\n",
    "    \"\"\"Creates Keras model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_dense, input_dim=input_dim,activation='relu'))\n",
    "    model.add(Dense(1,activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "nn_cv_pred=cv(keras_fit_predict,get_model,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train gradient boosted trees in a CV scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 200 rounds.\n",
      "[0]\ttrain-auc:0.779680\teval-auc:0.752137\n",
      "[500]\ttrain-auc:0.986674\teval-auc:0.926433\n",
      "[1000]\ttrain-auc:0.994557\teval-auc:0.929153\n",
      "[1500]\ttrain-auc:0.997449\teval-auc:0.929363\n",
      "Stopping. Best iteration:\n",
      "[1355]\ttrain-auc:0.996846\teval-auc:0.929613\n",
      "\n",
      "Will train until eval error hasn't decreased in 200 rounds.\n",
      "[0]\ttrain-auc:0.784438\teval-auc:0.766076\n",
      "[500]\ttrain-auc:0.986822\teval-auc:0.928112\n",
      "[1000]\ttrain-auc:0.994702\teval-auc:0.929339\n",
      "Stopping. Best iteration:\n",
      "[1266]\ttrain-auc:0.996545\teval-auc:0.929783\n",
      "\n",
      "Will train until eval error hasn't decreased in 200 rounds.\n",
      "[0]\ttrain-auc:0.779405\teval-auc:0.763856\n",
      "[500]\ttrain-auc:0.987058\teval-auc:0.931594\n",
      "[1000]\ttrain-auc:0.994787\teval-auc:0.932908\n",
      "Stopping. Best iteration:\n",
      "[844]\ttrain-auc:0.993201\teval-auc:0.933077\n",
      "\n",
      "Will train until eval error hasn't decreased in 200 rounds.\n",
      "[0]\ttrain-auc:0.781964\teval-auc:0.763041\n",
      "[500]\ttrain-auc:0.987058\teval-auc:0.926184\n",
      "[1000]\ttrain-auc:0.994780\teval-auc:0.928194\n",
      "Stopping. Best iteration:\n",
      "[1089]\ttrain-auc:0.995451\teval-auc:0.928441\n",
      "\n",
      "Will train until eval error hasn't decreased in 200 rounds.\n",
      "[0]\ttrain-auc:0.787721\teval-auc:0.759659\n",
      "[500]\ttrain-auc:0.986806\teval-auc:0.926446\n",
      "[1000]\ttrain-auc:0.994789\teval-auc:0.929230\n",
      "[1500]\ttrain-auc:0.997593\teval-auc:0.929529\n",
      "Stopping. Best iteration:\n",
      "[1332]\ttrain-auc:0.996940\teval-auc:0.929802\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#model params\n",
    "params = {'max_depth':20,\n",
    "         'eta':0.1,\n",
    "         'min_child_weight':5,\n",
    "         'colsample_bytree':1,\n",
    "         'subsample':1,\n",
    "         'silent':1,\n",
    "         'objective': \"binary:logistic\",\n",
    "         'eval_metric': 'auc',\n",
    "         'nthread':4}\n",
    "\n",
    "xgb_cv_pred=cv(xgb_fit_predict,params,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a stacked ensemble of my NN and boosted tree models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_stacked=np.column_stack([X,xgb_cv_pred,nn_cv_pred])\n",
    "\n",
    "params = {'max_depth':2,\n",
    "         'eta':0.1,\n",
    "         'min_child_weight':5,\n",
    "         'colsample_bytree':1,\n",
    "         'subsample':1,\n",
    "         'silent':1,\n",
    "         'objective': \"binary:logistic\",\n",
    "         'eval_metric': 'auc',\n",
    "         'nthread':8}\n",
    "\n",
    "stacked_cv_pred=cv(xgb_fit_predict,params,X_stacked,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NetMHCpan predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_from_fname(fname):\n",
    "    \"\"\"Get allele from the filename\"\"\"\n",
    "    # a bit convoluted naming\n",
    "    return '_'.join(os.path.basename(fname).split('rnd.')[-1].split('.')[0].split('-')[:-1])\n",
    "\n",
    "def load_allele_table(fname,allele):\n",
    "    \"\"\"Load a result table and annotate with allele\"\"\"\n",
    "    #note they are .xls but csv files...\n",
    "    tmp_table=pd.read_csv(fname,sep='\\t')\n",
    "    tmp_table['allele']=allele\n",
    "    tmp_table['len']=[len(x) for x in tmp_table.sequence]\n",
    "    tmp_table['binding']=(tmp_table.meas <= np.log10(500)).astype('int')\n",
    "    return tmp_table\n",
    "\n",
    "def load_all_tables(path):\n",
    "    \"\"\"Load all tables in directory and concat them into one big table\"\"\"\n",
    "    return pd.concat([load_allele_table(f,all_from_fname(f)) for f in glob.glob(path)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load cv tables\n",
    "netmhc_cv_res=load_all_tables('benchmark_mhci_reliability/predictions/cv_rnd/netmhcpan/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"NN AUC:\",roc_auc_score(y,nn_cv_pred)\n",
    "print \"XGB AUC:\",roc_auc_score(y,xgb_cv_pred)\n",
    "print \"ENSEMBLE AUC:\",roc_auc_score(y,stacked_cv_pred)\n",
    "print \"NetMHCpan AUC:\", roc_auc_score(netmhc_cv_res.binding,-netmhc_cv_res.pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
